{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770fc6e0-28f0-4b44-9c7a-db10ad959a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0986b159-ce9d-4607-aefb-8ce5f1a6239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import wikipedia\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd847ad2-4c4d-4bdf-b730-5b7f0e177c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE_PATH = 'Cities_data.csv'\n",
    "OUTPUT_PICKLE_FILE = 'wiki_city_content.pkl'\n",
    "OUTPUT_JSON_FILE = 'wiki_city_content.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958ed6e4-c56c-4577-a5c7-5e9153e06b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 unique city names in Cities_data.csv.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "cities = df['City'].unique().tolist()\n",
    "print(f\"Found {len(cities)} unique city names in {CSV_FILE_PATH}.\")\n",
    "wikipedia.set_lang('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbe1278-632c-4e8c-8b7d-6738ac94722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched content for Honolulu.\n",
      "Successfully fetched content for San Francisco.\n",
      "Successfully fetched content for Dubai.\n",
      "Successfully fetched content for Los Angeles.\n",
      "Successfully fetched content for Perth.\n",
      "Successfully fetched content for Melbourne.\n",
      "Successfully fetched content for Singapore.\n",
      "Successfully fetched content for Sydney.\n",
      "Successfully fetched content for Miami.\n",
      "Successfully fetched content for Copenhagen.\n",
      "Successfully fetched content for Lisbon.\n",
      "Successfully fetched content for Dallas.\n",
      "Successfully fetched content for Madrid.\n",
      "Successfully fetched content for Luxembourg.\n",
      "Successfully fetched content for Geneva.\n",
      "Successfully fetched content for Frankfurt.\n",
      "Successfully fetched content for Christchurch.\n",
      "Successfully fetched content for Amsterdam.\n",
      "Successfully fetched content for Munich.\n",
      "Successfully fetched content for Barcelona.\n",
      "Successfully fetched content for Wellington.\n",
      "Successfully fetched content for Auckland.\n",
      "Successfully fetched content for Helsinki.\n",
      "Successfully fetched content for Stockholm.\n",
      "Successfully fetched content for Nice.\n",
      "Successfully fetched content for Oslo.\n",
      "Successfully fetched content for Rome.\n",
      "Successfully fetched content for Athens.\n",
      "Successfully fetched content for Zurich.\n",
      "Successfully fetched content for London.\n",
      "Successfully fetched content for Paris.\n",
      "Successfully fetched content for Osaka.\n",
      "Successfully fetched content for Tokyo.\n",
      "Successfully fetched content for Vienna.\n",
      "Successfully fetched content for Cologne.\n",
      "Successfully fetched content for NYC.\n",
      "Successfully fetched content for Berlin.\n",
      "Successfully fetched content for Lyon.\n",
      "Successfully fetched content for Vancouver.\n",
      "Successfully fetched content for Manchester.\n",
      "Successfully fetched content for Glasgow.\n",
      "Successfully fetched content for Brussels.\n",
      "Successfully fetched content for Toronto.\n",
      "Successfully fetched content for Milan.\n",
      "Successfully fetched content for Hyderabad.\n",
      "Successfully fetched content for Buenos Aires.\n",
      "Successfully fetched content for Bengaluru.\n",
      "Successfully fetched content for Seoul.\n",
      "Successfully fetched content for Mumbai.\n",
      "Successfully fetched content for Delhi.\n"
     ]
    }
   ],
   "source": [
    "wiki_content = {}\n",
    "for city in cities:\n",
    "        page = wikipedia.page(city, auto_suggest=False, redirect=True)\n",
    "        wiki_content[city] = page.content\n",
    "        print(f\"Successfully fetched content for {city}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1be17f8-0d0f-4385-bfab-36d14e3d7d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved fetched content to wiki_city_content.pkl\n",
      "\n",
      "Successfully saved fetched content to wiki_city_content.json\n"
     ]
    }
   ],
   "source": [
    "if wiki_content:\n",
    "    with open(OUTPUT_PICKLE_FILE, 'wb') as f:\n",
    "        pickle.dump(wiki_content, f)\n",
    "        print(f\"\\nSuccessfully saved fetched content to {OUTPUT_PICKLE_FILE}\")\n",
    "\n",
    "    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(wiki_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\nSuccessfully saved fetched content to {OUTPUT_JSON_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6e5434-822c-4f8a-89bf-c27b8eca581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PICKLE_FILE = 'wiki_city_content.pkl'\n",
    "OUTPUT_PROCESSED_PICKLE_FILE = 'wiki_preprocessed.pkl'\n",
    "OUTPUT_PROCESSED_JSON_FILE = 'wiki_preprocessed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406ba6fc-5e88-4341-8248-56136f0b9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096b0adb-e2bf-4057-9be0-49f0c5b8a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_PICKLE_FILE, 'rb') as f:\n",
    "    wiki_content = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef86f55-3450-4371-89e7-51497733b915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 'Honolulu' (23757 chars)\n",
      "  Processed 'San Francisco' (64445 chars)\n",
      "  Processed 'Dubai' (57019 chars)\n",
      "  Processed 'Los Angeles' (44233 chars)\n",
      "  Processed 'Perth' (39187 chars)\n",
      "  Processed 'Melbourne' (47340 chars)\n",
      "  Processed 'Singapore' (59396 chars)\n",
      "  Processed 'Sydney' (64380 chars)\n",
      "  Processed 'Miami' (40438 chars)\n",
      "  Processed 'Copenhagen' (54130 chars)\n",
      "  Processed 'Lisbon' (43196 chars)\n",
      "  Processed 'Dallas' (58581 chars)\n",
      "  Processed 'Madrid' (59589 chars)\n",
      "  Processed 'Luxembourg' (35672 chars)\n",
      "  Processed 'Geneva' (45095 chars)\n",
      "  Processed 'Frankfurt' (79882 chars)\n",
      "  Processed 'Christchurch' (45254 chars)\n",
      "  Processed 'Amsterdam' (57163 chars)\n",
      "  Processed 'Munich' (53404 chars)\n",
      "  Processed 'Barcelona' (44856 chars)\n",
      "  Processed 'Wellington' (41842 chars)\n",
      "  Processed 'Auckland' (37688 chars)\n",
      "  Processed 'Helsinki' (41299 chars)\n",
      "  Processed 'Stockholm' (43206 chars)\n",
      "  Processed 'Nice' (26793 chars)\n",
      "  Processed 'Oslo' (38653 chars)\n",
      "  Processed 'Rome' (54023 chars)\n",
      "  Processed 'Athens' (40203 chars)\n",
      "  Processed 'Zurich' (43072 chars)\n",
      "  Processed 'London' (64219 chars)\n",
      "  Processed 'Paris' (59188 chars)\n",
      "  Processed 'Osaka' (33792 chars)\n",
      "  Processed 'Tokyo' (49737 chars)\n",
      "  Processed 'Vienna' (51889 chars)\n",
      "  Processed 'Cologne' (34619 chars)\n",
      "  Processed 'NYC' (63941 chars)\n",
      "  Processed 'Berlin' (52163 chars)\n",
      "  Processed 'Lyon' (26158 chars)\n",
      "  Processed 'Vancouver' (46073 chars)\n",
      "  Processed 'Manchester' (40970 chars)\n",
      "  Processed 'Glasgow' (62159 chars)\n",
      "  Processed 'Brussels' (55480 chars)\n",
      "  Processed 'Toronto' (56610 chars)\n",
      "  Processed 'Milan' (59177 chars)\n",
      "  Processed 'Hyderabad' (39123 chars)\n",
      "  Processed 'Buenos Aires' (57551 chars)\n",
      "  Processed 'Bengaluru' (39062 chars)\n",
      "  Processed 'Seoul' (29474 chars)\n",
      "  Processed 'Mumbai' (47687 chars)\n",
      "  Processed 'Delhi' (35659 chars)\n"
     ]
    }
   ],
   "source": [
    "preprocessed_content = {}\n",
    "for city, text in wiki_content.items():\n",
    "    tokens = word_tokenize(text.lower())    \n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stop_words and token not in string.punctuation:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "    preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "    preprocessed_content[city] = preprocessed_text\n",
    "    print(f\"  Processed '{city}' ({len(preprocessed_text)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188040e9-fcbf-4d49-a6b3-98874f4d0fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved preprocessed content to wiki_preprocessed.pkl\n",
      "Successfully saved preprocessed content to wiki_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_PROCESSED_PICKLE_FILE, 'wb') as f:\n",
    "    pickle.dump(preprocessed_content, f)\n",
    "    print(f\"\\nSuccessfully saved preprocessed content to {OUTPUT_PROCESSED_PICKLE_FILE}\")\n",
    "\n",
    "with open(OUTPUT_PROCESSED_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(preprocessed_content, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Successfully saved preprocessed content to {OUTPUT_PROCESSED_JSON_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155de461-2a5e-4867-bf92-4782b14ca273",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROCESSED_PICKLE_FILE = 'wiki_preprocessed.pkl'\n",
    "OUTPUT_VECTORIZER_FILE = 'tfidf_vectorizer.joblib'\n",
    "OUTPUT_MATRIX_FILE = 'tfidf_matrix.npz'\n",
    "OUTPUT_CITY_ORDER_FILE = 'city_order.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746a747f-5a3f-4716-8197-853ed79c092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_PROCESSED_PICKLE_FILE, 'rb') as f:\n",
    "    preprocessed_content = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7d6fd6-31ba-4782-9d06-60a47dc6c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_order = df['City'].unique().tolist()\n",
    "texts_in_order = []\n",
    "for city in city_order:\n",
    "    if city in preprocessed_content:\n",
    "        texts_in_order.append(preprocessed_content[city])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90e5f92-202d-4a1b-b06c-7d9c780f7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix created successfully.\n",
      "Shape of TF-IDF matrix: (50, 17846)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=3, ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(texts_in_order)\n",
    "print(\"TF-IDF matrix created successfully.\")\n",
    "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14faaa9a-686a-4fa3-a800-ff6a6b7b374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer saved to tfidf_vectorizer.joblib\n",
      "TF-IDF matrix saved to tfidf_matrix.npz\n",
      "City order saved to city_order.pkl\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(vectorizer, OUTPUT_VECTORIZER_FILE)\n",
    "print(f\"Vectorizer saved to {OUTPUT_VECTORIZER_FILE}\")\n",
    "save_npz(OUTPUT_MATRIX_FILE, tfidf_matrix)\n",
    "print(f\"TF-IDF matrix saved to {OUTPUT_MATRIX_FILE}\")\n",
    "with open(OUTPUT_CITY_ORDER_FILE, 'wb') as f:\n",
    "    pickle.dump(city_order, f)\n",
    "print(f\"City order saved to {OUTPUT_CITY_ORDER_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1549b85-543d-4101-a7c6-1229fdfcc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER_FILE = 'tfidf_vectorizer.joblib'\n",
    "MATRIX_FILE = 'tfidf_matrix.npz'\n",
    "CITY_ORDER_FILE = 'city_order.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb56eca-0f0b-4b65-a0a1-d6ccb0b45f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_cities(user_query):\n",
    "\n",
    "    if not user_query:\n",
    "        print(\"Error: No keywords entered.\")\n",
    "        return\n",
    "    \n",
    "    text = user_query\n",
    "    tokens = word_tokenize(text.lower())    \n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stop_words and token not in string.punctuation:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "    processed_query = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    if processed_query:\n",
    "        print(f\"Processed query: '{processed_query}'\")\n",
    "    \n",
    "        query_vector = vectorizer.transform([processed_query])\n",
    "        print(f\"\\nQuery transformed to vector shape: {query_vector.shape}\")\n",
    "    \n",
    "        cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "        city_scores = list(zip(city_order, cosine_similarities))\n",
    "        ranked_cities = sorted(city_scores, key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "        print(\"\\nRanking complete.\")\n",
    "        return ranked_cities\n",
    "\n",
    "    print(\"Query does not have any significant words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6315f646-7aca-4164-b9db-893cf7f9d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter keywords describing your ideal city (e.g., beaches nightlife history tech):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No keywords entered.\n"
     ]
    }
   ],
   "source": [
    "user_keywords = input(\"Enter keywords describing your ideal city (e.g., beaches nightlife history tech): \")\n",
    "results = find_similar_cities(user_keywords)\n",
    "if results:\n",
    "    print(\"\\n--- Top Matching Cities ---\")\n",
    "    for i, (city, score) in enumerate(results[:10]):\n",
    "        print(f\"{i+1}. {city} (Similarity Score: {score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
